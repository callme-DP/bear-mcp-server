from neo4j import GraphDatabase
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import os

# ========== Neo4j è¿æ¥é…ç½® ==========
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "your_password"  # â† æ›¿æ¢ä¸ºä½ å®é™…å¯†ç 

EXPORT_PATH = "src/exports"

# ========== è¯»å–æ•°æ® ==========
with open(os.path.join(EXPORT_PATH, "graph.json"), "r", encoding="utf-8") as f:
    graph_data = json.load(f)

with open(os.path.join(EXPORT_PATH, "meta.json"), "r", encoding="utf-8") as f:
    meta_raw = json.load(f)

# meta.json å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œä¹Ÿå¯èƒ½æ˜¯å­—å…¸ï¼Œæˆ‘ä»¬ç»Ÿä¸€æˆå­—å…¸æ ¼å¼
if isinstance(meta_raw, list):
    meta_data = {str(item.get("id")): item for item in meta_raw if item.get("id")}
else:
    meta_data = meta_raw

# å¦‚æœæœ‰ embeddings.json å°±åŠ è½½
embeddings = {}
embed_path = os.path.join(EXPORT_PATH, "embeddings.json")
if os.path.exists(embed_path):
    with open(embed_path, "r", encoding="utf-8") as f:
        embeddings_raw = json.load(f)

    # âœ… case1: list[list[float]]
    if isinstance(embeddings_raw, list):
        print(f"ğŸ§© embeddings.json æ£€æµ‹ä¸º listï¼Œå…± {len(embeddings_raw)} æ¡è®°å½•")
        node_list = graph_data.get("nodes", [])
        valid_len = min(len(node_list), len(embeddings_raw))

        for i in range(valid_len):
            nid = str(node_list[i].get("id"))
            emb = embeddings_raw[i]
            if emb and isinstance(emb, list):
                embeddings[nid] = emb

        print(f"âœ… å·²æˆåŠŸç»‘å®š {len(embeddings)} æ¡èŠ‚ç‚¹ embeddingï¼ˆè‡ªåŠ¨æŒ‰é¡ºåºå¯¹é½ï¼‰")

    # âœ… case2: dict{id: [float,...]}
    elif isinstance(embeddings_raw, dict):
        print(f"ğŸ§© embeddings.json æ£€æµ‹ä¸º dict æ ¼å¼ï¼Œå…± {len(embeddings_raw)} æ¡")
        embeddings = embeddings_raw

    else:
        print("âš ï¸ æ— æ³•è¯†åˆ« embeddings.json æ ¼å¼ï¼Œä¿æŒä¸ºç©º")

else:
    print("âš ï¸ æœªæ‰¾åˆ° embeddings.json æ–‡ä»¶ï¼Œè·³è¿‡")


nodes = graph_data["nodes"]
edges = graph_data["edges"]
# ========== æ—¥å¿—è¾…åŠ©å‡½æ•° ==========
def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
# ======= æ™ºèƒ½æ£€æµ‹è¾“å‡ºæ—¥å¿— =======
def log_data_structure():
    log("\nğŸ“Š ====== æ•°æ®ç»“æ„æ£€æµ‹æŠ¥å‘Š ======")

    # æ£€æŸ¥ graph.json
    node_count = len(nodes) if isinstance(nodes, list) else 0
    edge_count = len(edges) if isinstance(edges, list) else 0
    log(f"ğŸ“˜ graph.json: èŠ‚ç‚¹ {node_count} ä¸ªï¼Œè¾¹ {edge_count} æ¡")

    # æ£€æŸ¥ meta.json
    if isinstance(meta_raw, list):
        log(f"ğŸŸ¢ meta.json: list æ ¼å¼ â†’ å·²è‡ªåŠ¨è½¬æ¢ä¸º dictï¼ˆå…± {len(meta_raw)} æ¡è®°å½•ï¼‰")
    elif isinstance(meta_raw, dict):
        log(f"ğŸŸ¢ meta.json: dict æ ¼å¼ï¼ˆå…± {len(meta_raw)} æ¡è®°å½•ï¼‰")
    else:
        log("âš ï¸ meta.json æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥å†…å®¹")

    # æ£€æŸ¥ embeddings.json
    if embeddings:
        log(f"ğŸ§  embeddings.json: å« {len(embeddings)} æ¡å‘é‡è®°å½•")
    else:
        log("âš ï¸ æœªæ£€æµ‹åˆ° embeddings.json æˆ–ä¸ºç©º")

    log("=================================\n")

# æ‰§è¡Œæ£€æµ‹æ—¥å¿—
log_data_structure()

# ========== è¿æ¥ Neo4j ==========
# auth=(NEO4J_USER, NEO4J_PASSWORD) -> NONE
driver = GraphDatabase.driver(NEO4J_URI, auth = None)

def detect_para_type(tags):
    """
    æ ¹æ®æ ‡ç­¾è‡ªåŠ¨è¯†åˆ« PARA ç±»å‹ï¼ˆé¡¹ç›®-é¢†åŸŸ-èµ„æº-å½’æ¡£ç³»ç»Ÿï¼‰
    
    åŸºäºæ ‡ç­¾å†…å®¹åˆ¤æ–­ç¬”è®°æ‰€å±çš„PARAåˆ†ç±»ï¼Œæ”¯æŒè¯†åˆ«é¡¹ç›®ã€é¢†åŸŸã€èµ„æºã€å½’æ¡£ã€æ ‡ç­¾å’Œæ™®é€šç¬”è®°ç±»å‹ã€‚

    Args:
        tags: æ ‡ç­¾åˆ—è¡¨ï¼ŒåŒ…å«ç¬”è®°çš„æ‰€æœ‰æ ‡ç­¾ä¿¡æ¯

    Returns:
        str: è¿”å›è¯†åˆ«å‡ºçš„PARAç±»å‹ï¼Œå¯èƒ½å€¼ä¸ºï¼š
            - "Project": é¡¹ç›®ç±»å‹
            - "Area": é¢†åŸŸç±»å‹  
            - "Resource": èµ„æºç±»å‹
            - "Archive": å½’æ¡£ç±»å‹
            - "Tag": æ ‡ç­¾ç±»å‹
            - "Note": æ™®é€šç¬”è®°ç±»å‹
    """
    """æ ¹æ®æ ‡ç­¾è‡ªåŠ¨è¯†åˆ« PARA ç±»å‹"""
    if not tags:
        return "Note"
    tags_str = " ".join(tags).lower()
    if "project" in tags_str:
        return "Project"
    elif "area" in tags_str:
        return "Area"
    elif "resource" in tags_str:
        return "Resource"
    elif "archive" in tags_str:
        return "Archive"
    elif any(t.startswith("#") for t in tags) or "tag" in tags_str:   # âœ… æ–°å¢ï¼šè¯†åˆ« Tag
        return "Tag"
    return "Note"

# ========== å¯¼å…¥å‡½æ•° ==========
def import_data(tx, nodes, edges, meta, embeds):
    # ======= æ•°æ®æ£€æµ‹ =======
    invalid_edges = [e for e in edges if "source" not in e or "target" not in e]
    if invalid_edges:
        print("âš ï¸ ç¤ºä¾‹é—®é¢˜è¾¹ï¼š", invalid_edges[:3])

    # ======= å¯¼å…¥èŠ‚ç‚¹ =======
    for n in nodes:
        nid = str(n["id"])
        meta_info = meta.get(nid, {})
        embedding = embeds.get(nid)

        # è¯†åˆ«æ²¡æœ‰ meta çš„æ ‡ç­¾èŠ‚ç‚¹
        is_tag = 0
        if not meta_info and nid.startswith("tag:"):
            is_tag = 1
            meta_info = {  # ç»™å®ƒä¸€ä¸ªæœ€ç®€é»˜è®¤meta
                "title": nid.replace("tag:", ""),
                "tags": ["tag"],
                "source": "graph_only"
            }
            
        # âœ… emojiæ ‡é¢˜ å®‰å…¨å¤„ç†
        safe_title = meta_info.get("title") or n.get("label") or ""
        if not isinstance(safe_title, str):
            safe_title = str(safe_title)
        safe_title = safe_title.encode("utf-8", "surrogatepass").decode("utf-8")

        
        # âœ… PARA åˆ†ç±»æ£€æµ‹
        tags = meta_info.get("tags", [])
        para_type = detect_para_type(tags)
        
        #âœ… æ·»åŠ urlå±æ€§
        bear_url = f"bear://x-callback-url/open-note?id={nid}"

       # âœ… æ”¹è¿›ç‰ˆï¼šè®© PARA ç±»å‹æˆä¸ºæ ‡ç­¾
        tx.run(f"""
            MERGE (a:Note:{para_type} {{id: $id}})
            SET a.title = $title,
                a.path = $path,
                a.tags = $tags,
                a.source = $source,
                a.embedding = $embedding,
                a.para_type = $para_type,
                a.isTag = $isTag,              
                a.bear_url = $bear_url,
                a.createdAt = datetime($createdAt),
                a.updatedAt = datetime($updatedAt)
        """,
            id=nid,
            title=safe_title,
            path=meta_info.get("path"),
            tags=tags,
            source=meta_info.get("source"),
            embedding=embedding,
            para_type=para_type,
            isTag=is_tag,  
            bear_url=bear_url,
            createdAt=n.get("created"),
            updatedAt=n.get("modified")
        )

# ========== å¯¼å…¥å‡½æ•° ==========
def import_data(tx, nodes, edges, meta, embeds):
    # ======= ç¤ºä¾‹æ‰“å° =======
    invalid_edges = [e for e in edges if "source" not in e or "target" not in e]
    if invalid_edges:
        print("âš ï¸ ç¤ºä¾‹é—®é¢˜è¾¹ï¼š", invalid_edges[:3])

    # ======= å¯¼å…¥èŠ‚ç‚¹ =======
    for n in nodes:
        nid = str(n["id"])
        meta_info = meta.get(nid, {})
        embedding = embeds.get(nid)

        # è¯†åˆ«æ²¡æœ‰ meta çš„æ ‡ç­¾èŠ‚ç‚¹
        is_tag = 0
        if not meta_info and nid.startswith("tag:"):
            is_tag = 1
            meta_info = {  # ç»™å®ƒä¸€ä¸ªæœ€ç®€é»˜è®¤meta
                "title": nid.replace("tag:", ""),
                "tags": ["tag"],
                "source": "graph_only"
            }

            
        # âœ… emoji å®‰å…¨å¤„ç†
        safe_title = meta_info.get("title") or n.get("label") or ""
        if not isinstance(safe_title, str):
            safe_title = str(safe_title)
        safe_title = safe_title.encode("utf-8", "surrogatepass").decode("utf-8")
        
        # âœ… PARA åˆ†ç±»æ£€æµ‹
        tags = meta_info.get("tags", [])
        para_type = detect_para_type(tags)
        
        #âœ… æ·»åŠ urlå±æ€§
        bear_url = f"bear://x-callback-url/open-note?id={nid}"

       # âœ… æ”¹è¿›ç‰ˆï¼šè®© PARA ç±»å‹æˆä¸ºæ ‡ç­¾
        tx.run(f"""
            MERGE (a:Note:{para_type} {{id: $id}})
            SET a.title = $title,
                a.path = $path,
                a.tags = $tags,
                a.source = $source,
                a.embedding = $embedding,
                a.para_type = $para_type,
                a.isTag = $isTag,              
                a.bear_url = $bear_url,
                a.createdAt = datetime($createdAt),
                a.updatedAt = datetime($updatedAt)
        """,
            id=nid,
            title=safe_title,
            path=meta_info.get("path"),
            tags=tags,
            source=meta_info.get("source"),
            embedding=embedding,
            para_type=para_type,
            isTag=is_tag,  
            bear_url=bear_url,
            createdAt=n.get("created"),
            updatedAt=n.get("modified")
        )

    # ======= å¯¼å…¥è¾¹å…³ç³»ï¼ˆå«è¯­ä¹‰å¢å¼ºï¼‰ =======
    for e in edges:
        # åŸºç¡€éªŒè¯
        if not isinstance(e, dict):
            print(f"âš ï¸ è·³è¿‡å¼‚å¸¸è¾¹ï¼ˆé dict ç±»å‹ï¼‰: {e}")
            continue
        if "from" not in e or "to" not in e:
            print(f"âš ï¸ è·³è¿‡ç¼ºå°‘ from/to å­—æ®µçš„è¾¹: {e}")
            continue

        source_id = str(e["from"])
        target_id = str(e["to"])
        weight = float(e.get("weight", 1.0))

        # ====== æ ¹æ®æ ‡ç­¾è¯†åˆ«å…³ç³»è¯­ä¹‰ ======
        source_meta = meta.get(source_id, {})
        target_meta = meta.get(target_id, {})
        source_tags = [t.lower() for t in meta.get(source_id, {}).get("tags", [])]
        target_tags = [t.lower() for t in meta.get(target_id, {}).get("tags", [])]
       
        # # é»˜è®¤å…³ç³»ç±»å‹,é»˜è®¤ç°è‰²
        rel_type,color = "RELATED","#9ca3af"    

        if any("area" in t for t in target_tags):
            rel_type = "BELONGS_TO"
            color = "#3b82f6"  # è“
        elif any("resource" in t for t in target_tags):
            rel_type = "CITES"
            color = "#f59e0b"  # æ©™
        elif any("concept" in t for t in target_tags):
            rel_type = "EXTENDS"
            color = "#8b5cf6"  # ç´«
        elif target_meta.get("para_type") == "Tag" or any("tag" in t for t in target_tags):  # âœ… æ–°å¢ï¼šæ ‡ç­¾è¯­ä¹‰è¾¹
            rel_type = "HAS_TAG"
            color = "#10b981"  # ç»¿

        # ====== åˆ›å»ºè¯­ä¹‰å…³ç³» ======
        try:
            tx.run(f"""
                MATCH (a:Note {{id: $source}})
                MATCH (b:Note {{id: $target}})
                MERGE (a)-[r:{rel_type} {{
                    weight: $weight,
                    rel_type: $rel_type,
                    color: $color
                }}]->(b)
            """, 
                source=source_id,
                target=target_id,
                weight=weight,
                rel_type=rel_type,
                color=color
            )
        except Exception as ex:
            print(f"âŒ åˆ›å»ºè¾¹å¤±è´¥: {e}, é”™è¯¯ä¿¡æ¯: {ex}")

    print(f"\nâœ… æˆåŠŸå¯¼å…¥ {len(edges)} æ¡è¾¹ï¼ˆå¼‚å¸¸æ¡ç›®å·²è·³è¿‡ï¼‰\n")
     # ======= è¯­ä¹‰æ¦‚å¿µèšç±»ï¼ˆConcept ç”Ÿæˆï¼‰ =======
    try:
        print("\nğŸ§  å¼€å§‹åŸºäºèŠ‚ç‚¹ embedding ç”Ÿæˆ Concept èšç±»...")

        from sklearn.cluster import DBSCAN
        from sklearn.preprocessing import normalize
        from sklearn.metrics.pairwise import cosine_similarity
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        import re

        # 1ï¸âƒ£ è¿‡æ»¤æ‰ä¸€çº§æ ‡ç­¾ï¼ˆarea/project/reflect/concept/resourceï¼‰
        exclude_tags = {"area", "project", "reflect", "concept", "resource"}
        filtered_embeds = {
            nid: emb for nid, emb in embeds.items()
            if emb is not None and all(
                t not in exclude_tags for t in meta.get(nid, {}).get("tags", [])
            )
        }

        if not filtered_embeds:
            print("âš ï¸ æ— æœ‰æ•ˆ embeddingï¼Œå¯è·³è¿‡ Concept èšç±»")
            return

        node_ids = list(filtered_embeds.keys())
        vectors = np.array([np.array(filtered_embeds[i]) for i in node_ids])

        # 2ï¸âƒ£ å½’ä¸€åŒ–ï¼ˆé˜²æ­¢ cosine æŠ¥ Negative values é”™ï¼‰
        vectors = normalize(vectors, norm='l2')

        # 3ï¸âƒ£ DBSCAN èšç±»
        clustering = DBSCAN(eps=0.15, min_samples=2, metric="cosine")
        clustering.fit(vectors)
        labels = clustering.labels_

        clusters = {}
        for nid, label in zip(node_ids, labels):
            if label == -1:
                continue
            clusters.setdefault(label, []).append(nid)

        print(f"ğŸŒŸ å…±æ£€æµ‹åˆ° {len(clusters)} ä¸ªæ½œåœ¨ Concept èšç±»")

        # 4ï¸âƒ£ è®¡ç®—æ¯ä¸ªç°‡çš„å¹³å‡ç›¸ä¼¼åº¦
        sim_matrix = cosine_similarity(vectors)
        cluster_sims = {}
        for cid, ids in clusters.items():
            if len(ids) < 2:
                cluster_sims[cid] = 1.0
                continue
            idxs = [node_ids.index(i) for i in ids]
            avg_sim = float(np.mean(sim_matrix[np.ix_(idxs, idxs)]))
            cluster_sims[cid] = avg_sim
            print(f"ğŸ§© Concept_{cid}: å¹³å‡ç›¸ä¼¼åº¦ = {avg_sim:.3f}")

        # 5ï¸âƒ£ æ„é€  TF-IDF è¯­ä¹‰å‘½å + é¢œè‰² / type æ ‡æ³¨
        for cid, ids in clusters.items():
            corpus = []
            for i in ids:
                mi = meta.get(i, {})
                title = str(mi.get("title", "") or "")
                tags_list = mi.get("tags", [])
                tags = " ".join([t for t in tags_list if isinstance(t, str)])
                text = f"{title} {tags}".strip()
                text = re.sub(r"[^a-zA-Z\u4e00-\u9fa5\s]+", " ", text)
                if text.strip():
                    corpus.append(text)

            # ---- TF-IDF ----
            top_words = []
            if corpus:
                try:
                    vec = TfidfVectorizer(max_features=50)
                    X = vec.fit_transform(corpus)
                    mean_tfidf = np.asarray(X.mean(axis=0)).ravel()
                    vocab = np.array(vec.get_feature_names_out())
                    top_idx = mean_tfidf.argsort()[::-1][:3]
                    top_words = [w for w in vocab[top_idx] if w]
                except Exception as ex:
                    print(f"âš ï¸ TF-IDF å‘½åå¤±è´¥ ({ex})ï¼Œä½¿ç”¨é»˜è®¤åã€‚")

            # ---- èšç°‡è¯­ä¹‰å‘½å ----
            if top_words:
                raw_name = f"Concept_{cid}_" + "_".join(top_words)
            else:
                raw_name = f"Concept_{cid}_semantic_cluster"
            concept_name = re.sub(r"[^a-zA-Z0-9_\u4e00-\u9fa5]+", "_", raw_name)[:120]

            # ---- å¹³å‡ç›¸ä¼¼åº¦è¯„ä¼° ----
            avg_sim = cluster_sims.get(cid, 0)
            if avg_sim >= 0.85:
                color = "#bdbdbd"       # ç°è‰²ï¼šåˆç†èšç±»
                semantic_type = "semantic_cluster"
            else:
                color = "#f6c344"       # é»„è‰²ï¼šè¯­ä¹‰æ¼‚ç§»
                semantic_type = "semantic_resource"

            # ---- å†™å…¥ Concept èŠ‚ç‚¹ ----
            concept_emb = np.mean(
                [filtered_embeds[i] for i in ids if i in filtered_embeds], axis=0
            ).tolist()

            tx.run("""
                MERGE (c:Concept {name:$name})
                SET c.color=$color,
                    c.type=$type,
                    c.similarity=$sim,
                    c.embedding=$embedding,
                    c.createdAt=datetime()
            """, name=concept_name, color=color, type=semantic_type,
                sim=avg_sim, embedding=concept_emb)

            # ---- å»ºç«‹ Note â†’ Concept å…³ç³» ----
            for nid in ids:
                tx.run("""
                    MATCH (n:Note {id:$nid}), (c:Concept {name:$cname})
                    MERGE (n)-[:EXTENDS {weight:1.0, color:$color}]->(c)
                """, nid=nid, cname=concept_name, color=color)

        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(clusters)} ä¸ª Concept èŠ‚ç‚¹å¹¶å»ºç«‹è¯­ä¹‰å…³è”ã€‚\n")


    except Exception as ex:
        print(f"âŒ Concept èšç±»é˜¶æ®µå‡ºé”™: {ex}")


    print(f"\nâœ… æˆåŠŸå¯¼å…¥ {len(edges)} æ¡è¾¹ï¼ˆå¼‚å¸¸æ¡ç›®å·²è·³è¿‡ï¼‰\n")



# ========== æ‰§è¡Œå¯¼å…¥ ==========
with driver.session() as session:
    session.execute_write(import_data, nodes, edges, meta_data, embeddings)

log("âœ… å¯¼å…¥æˆåŠŸï¼Œæ‰€æœ‰ç¬”è®°ä¸å‘é‡å·²å¯¼å…¥ Neo4jï¼")
